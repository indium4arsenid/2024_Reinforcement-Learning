{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rr1d-kHohxGL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[accept-rom-license,atari]==1.0.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (1.0.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: ale-py==0.9.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license,atari]==1.0.0) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license,atari]==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license,atari]==1.0.0) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license,atari]==1.0.0) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[atari,accept-rom-license]==1.0.0\" \"ale-py==0.9.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "from gymnasium.spaces import Box\n",
    "from collections import deque\n",
    "import copy\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "import ale_py\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# EXTRA ADDITIONS BLOCK\n",
    "# -------------------------------\n",
    "\n",
    "# Add TimeLimit wrapper (EXTRA)\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "# Add hyperparameter search tools (EXTRA)\n",
    "from itertools import product\n",
    "\n",
    "# Matplotlib settings for saving high-quality plots (EXTRA)\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "\n",
    "# Register Atari environments from ale_py (EXTRA)\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Set global seed for reproducibility (EXTRA)\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Device setup for GPU/CPU compatibility (EXTRA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IGCa_JQeiy1F"
   },
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, num_skip):\n",
    "        super().__init__(env)\n",
    "        self.num_skip = num_skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self.num_skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        return obs, total_reward, terminated, truncated, info\n",
    "\n",
    "#ORIGINAL\n",
    "#class GrayScaleObservation(gym.ObservationWrapper):\n",
    " #   def __init__(self, env):\n",
    "  #      super().__init__(env)\n",
    "   #     obs_shape = self.observation_space.shape[:2]\n",
    "    #    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.float32)\n",
    "\n",
    "#    def observation(self, observation):\n",
    " #       observation = np.transpose(observation, (2, 0, 1))\n",
    "  #      observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "   #     transform = torchvision.transforms.Grayscale()\n",
    "    #    observation = transform(observation)\n",
    "     #   return observation\n",
    "#ENHANCED PREPROCESSING\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, apply_grayscale=True):  # EXTRA\n",
    "        \"\"\"\n",
    "        Optionally converts observations to grayscale.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.apply_grayscale = apply_grayscale  # EXTRA\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        if self.apply_grayscale:  # EXTRA\n",
    "            self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Convert observation to grayscale if required.\n",
    "        \"\"\"\n",
    "        if self.apply_grayscale:  # EXTRA\n",
    "            observation = np.transpose(observation, (2, 0, 1))\n",
    "            observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "            transform = torchvision.transforms.Grayscale()\n",
    "            observation = transform(observation)\n",
    "        return observation\n",
    "   #ORIGINAL     \n",
    "#class ResizeObservation(gym.ObservationWrapper):\n",
    " #   def __init__(self, env, shape):\n",
    "  #      super().__init__(env)\n",
    "   #     self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
    "    #    obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "     #   self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.float32)\n",
    "\n",
    "    #def observation(self, observation):\n",
    "     #   transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n",
    "         #torchvision.transforms.Normalize(0, 255)])\n",
    "      #  return transforms(observation).squeeze(0)\n",
    "\n",
    "#ENHANCED PREPROCESSING\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape, normalize=True):  # EXTRA\n",
    "        \"\"\"\n",
    "        Resize observations to the specified shape and optionally normalize.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
    "        self.normalize = normalize  # EXTRA\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Resize observation to the target shape and normalize if required.\n",
    "        \"\"\"\n",
    "        transforms = [torchvision.transforms.Resize(self.shape)]\n",
    "        if self.normalize:  # EXTRA\n",
    "            transforms.append(torchvision.transforms.Normalize(0, 255))\n",
    "        transform_pipeline = torchvision.transforms.Compose(transforms)\n",
    "        return transform_pipeline(observation).squeeze(0)\n",
    "        \n",
    "class ExperienceReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def store(self, state, next_state, action, reward, terminated, truncated):\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        self.memory.append((state, next_state, action, reward, terminated, truncated))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # DONE\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        states, next_states, actions, rewards, terminated, truncated = zip(*experiences)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(np.array(states), dtype=torch.float32),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(terminated, dtype=torch.float32),\n",
    "            torch.tensor(truncated, dtype=torch.float32),\n",
    "        )\n",
    "# EXTRA: Flexible Preprocessing Pipeline Wrapper\n",
    "class FlexiblePreprocessingWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A flexible wrapper to combine grayscale, resizing, and normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, config):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: The environment to wrap.\n",
    "            config: Dictionary with preprocessing options:\n",
    "                - 'grayscale': Convert to grayscale.\n",
    "                - 'resize_shape': Resize to this shape (tuple or int).\n",
    "                - 'normalize': Normalize pixel values.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.config = config\n",
    "        if self.config.get(\"grayscale\", True):  # Apply grayscale if enabled\n",
    "            env = GrayScaleObservation(env, apply_grayscale=self.config[\"grayscale\"])\n",
    "        if \"resize_shape\" in self.config:  # Resize if specified\n",
    "            env = ResizeObservation(env, shape=self.config[\"resize_shape\"], normalize=self.config.get(\"normalize\", True))\n",
    "        self.env = env  # Wrapped environment\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LOCiUgfBjJYc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stacked frames: 4\n",
      "Resized observation space dimensionality: 84, 84\n",
      "Number of available actions by the agent: 4\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#ORIGINAL\n",
    "\n",
    "#seed = 957\n",
    "#np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#if torch.backends.cudnn.enabled:\n",
    "#    torch.backends.cudnn.benchmark = False\n",
    "#    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#gym.register_envs(ale_py)\n",
    "\n",
    "#env_rendering = False    # Set to False while training your model on Colab\n",
    "\n",
    "# Create and preprocess the Atari Breakout environment\n",
    "#if env_rendering:\n",
    "#    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False, render_mode=\"human\")\n",
    "#else:\n",
    "#    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False)\n",
    "\n",
    "#env = SkipFrame(env, num_skip=4)\n",
    "#env = GrayScaleObservation(env)\n",
    "#env = ResizeObservation(env, shape=84)\n",
    "#env = FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "#image_stack, h, w = env.observation_space.shape\n",
    "#num_actions = env.action_space.n\n",
    "#print(f'Number of stacked frames: {image_stack}')\n",
    "#print(f'Resized observation space dimensionality: {h}, {w}')\n",
    "#print(f'Number of available actions by the agent: {num_actions}')\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stacked frames: 4\n",
      "Resized observation space dimensionality: 84, 84\n",
      "Number of available actions by the agent: 4\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ENhanced preprocessing\n",
    "# Seed Initialization for Reproducibility\n",
    "seed = 957\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Register Atari Environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "env_rendering = False    # Set to False while training your model on Colab\n",
    "\n",
    "# -------------------------------\n",
    "# EXTRA: Configuration Setup\n",
    "# -------------------------------\n",
    "# Preprocessing and environment configuration\n",
    "env_config = {\n",
    "    \"num_skip\": 4,                # Number of frames to skip\n",
    "    \"grayscale\": True,            # Convert frames to grayscale\n",
    "    \"resize_shape\": 84,           # Resize frames to 84x84\n",
    "    \"normalize\": True,            # Normalize pixel values\n",
    "    \"stack_size\": 4,              # Number of frames to stack\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# EXTRA: Create and Preprocess the Environment\n",
    "# -------------------------------\n",
    "if env_rendering:\n",
    "    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False)\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "env = SkipFrame(env, num_skip=env_config[\"num_skip\"])\n",
    "env = FlexiblePreprocessingWrapper(env, config=env_config)  # EXTRA\n",
    "env = FrameStackObservation(env, stack_size=env_config[\"stack_size\"])\n",
    "\n",
    "# Environment Properties\n",
    "image_stack, h, w = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "print(f'Number of stacked frames: {image_stack}')\n",
    "print(f'Resized observation space dimensionality: {h}, {w}')\n",
    "print(f'Number of available actions by the agent: {num_actions}')\n",
    "\n",
    "# Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NNoUgNjujqRX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepQNet(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepQNet(torch.nn.Module):\n",
    "    def __init__(self, h, w, image_stack, num_actions):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        # DONE: create a convolutional neural network\n",
    "        self.conv1 = torch.nn.Conv2d(image_stack, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = torch.nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DONE: forward pass from the neural network\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)  # Output Q-values\n",
    "\n",
    "# DONE: create an online and target DQN (Hint: Use copy.deepcopy() and \n",
    "#       set requires_grad to False for the parameters of the target DQN)\n",
    "online_dqn = DeepQNet(h, w, image_stack, num_actions)\n",
    "target_dqn = copy.deepcopy(online_dqn)\n",
    "# Freeze target network parameters to prevent updates\n",
    "for param in target_dqn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "online_dqn.to(device)\n",
    "target_dqn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RO21LQJ6j0WC"
   },
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    return torch.tensor(x.__array__()).float()\n",
    "\n",
    "\n",
    "class AtariAgent:\n",
    "    def __init__(self, buffer, eps, eps_decay, min_eps, gamma, batch_size,\n",
    "                 online_dqn, target_dqn, run_as_ddqn, \n",
    "                 optimizer, criterion, device,\n",
    "                 max_train_frames, burn_in_phase, sync_target):\n",
    "\n",
    "        self.buffer = buffer\n",
    "        self.eps = eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.min_eps = min_eps\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.online_dqn = online_dqn\n",
    "        self.target_dqn = target_dqn\n",
    "        self.run_as_ddqn = run_as_ddqn\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.max_train_frames = max_train_frames\n",
    "        self.burn_in_phase = burn_in_phase\n",
    "        self.sync_target = sync_target\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "\n",
    "    def policy(self, state, is_training):\n",
    "\n",
    "        state = convert(state).unsqueeze(0).to(self.device)\n",
    "        if is_training and random.random() < self.eps:\n",
    "            return random.randint(0, num_actions - 1)  # Random action (exploration)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.online_dqn(state)\n",
    "            return torch.argmax(q_values).item()  # Greedy action (exploitation)\n",
    "\n",
    "        # DONE: Implement an epsilon-greedy policy\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, state, action, reward, next_state, truncated, terminated):\n",
    "        state, action, reward, next_state, truncated, terminated = [x.to(self.device) for x in \n",
    "                                (state, action, reward, next_state, truncated, terminated)]\n",
    "\n",
    "        # DONE: Compute the DQN (or DDQN) loss based on self.criterion\n",
    "        q_values = self.online_dqn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.run_as_ddqn:  # EXTRA: Double DQN\n",
    "                # Double DQN: Use online DQN to select action and target DQN to evaluate Q-value\n",
    "                best_next_actions = self.online_dqn(next_state).argmax(1)\n",
    "                max_next_q_values = self.target_dqn(next_state).gather(1, best_next_actions.unsqueeze(1)).squeeze(1)\n",
    "            else:\n",
    "                # Standard DQN\n",
    "                max_next_q_values = self.target_dqn(next_state).max(1)[0]\n",
    "\n",
    "            target_q_values = reward + self.gamma * max_next_q_values * (1 - terminated)\n",
    "\n",
    "        return self.criterion(q_values, target_q_values)\n",
    "\n",
    "\n",
    "    def run_episode(self, is_training):\n",
    "        episode_reward, episode_loss = 0, 0.\n",
    "        state, _ = env.reset(seed=seed)\n",
    "\n",
    "        for t in range(self.max_train_frames):\n",
    "            action = self.policy(state, is_training)\n",
    "            self.current_step += 1\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            if is_training:\n",
    "                self.buffer.store(state, next_state, action, reward, terminated, truncated)\n",
    "\n",
    "                if self.current_step > self.burn_in_phase:\n",
    "                    state_batch, next_state_batch, action_batch, \\\n",
    "                        reward_batch, terminated_batch, truncated_batch = self.buffer.sample(self.batch_size)\n",
    "\n",
    "                    if self.current_step % self.sync_target == 0:\n",
    "                        # DONE: Periodically update your target_dqn at each sync_target frames\n",
    "                        self.target_dqn.load_state_dict(self.online_dqn.state_dict())\n",
    "\n",
    "                    loss = self.compute_loss(state_batch, action_batch, reward_batch, \n",
    "                                             next_state_batch, terminated_batch, truncated_batch)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    episode_loss += loss.detach().item()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    st = convert(state).to(self.device).unsqueeze(0)\n",
    "                    next_st = convert(next_state).to(self.device).unsqueeze(0)\n",
    "                    act = action.to(self.device)\n",
    "                    rew = torch.tensor(reward).to(self.device)\n",
    "                    trunc = torch.tensor(truncated).to(self.device)\n",
    "                    term = torch.tensor(terminated).to(self.device)\n",
    "\n",
    "                    episode_loss += self.compute_loss(st, act, rew, next_st, term, trunc).item()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if self.current_step > self.burn_in_phase and self.eps > self.min_eps:\n",
    "                self.eps *= self.eps_decay\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        return dict(reward=episode_reward, loss=episode_loss / t)\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, train_metrics, save_filename):\n",
    "        save_dict = {'curr_step': self.current_step,\n",
    "                    'train_metrics': train_metrics,\n",
    "                    'eps': self.eps,\n",
    "                    'online_dqn': self.online_dqn.state_dict(),\n",
    "                    'target_dqn': self.target_dqn.state_dict()}\n",
    "\n",
    "        torch.save(save_dict, save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PEGSGYsQj8Wh"
   },
   "outputs": [],
   "source": [
    "def update_metrics(metrics, episode):\n",
    "    for k, v in episode.items():\n",
    "        metrics[k].append(v)\n",
    "\n",
    "\n",
    "def print_metrics(it, metrics, is_training, window=100):\n",
    "    reward_mean = np.mean(metrics['reward'][-window:])\n",
    "    loss_mean = np.mean(metrics['loss'][-window:])\n",
    "    mode = \"train\" if is_training else \"test\"\n",
    "    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdhWcoW-kyuF"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters (TODO: modify as needed)\n",
    "#batch_size = 32\n",
    "#alpha = 0.00025\n",
    "#gamma = 0.95\n",
    "#eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n",
    "#buffer = ExperienceReplayMemory(20_000)\n",
    "#burn_in_phase = 20_000\n",
    "#sync_target = 30_000\n",
    "#max_train_frames = 10_000\n",
    "#max_train_episodes = 100_000\n",
    "#max_test_episodes = 1\n",
    "#run_as_ddqn = False # Set the run_as_ddqn flag to True if you want to run the DDQN algorithm\n",
    "#save_filename = './saved_model.pt'\n",
    "\n",
    "# TODO: create the appropriate MSE criterion and Adam optimizer\n",
    "#optimizer = ...\n",
    "#criterion = ...\n",
    "\n",
    "#testing_mode = False # Change to True if you want to load a saved model\n",
    "\n",
    "#if testing_mode:\n",
    "    # TODO: Load your saved online_dqn model for testing. \n",
    "    #       The target_dqn should be the same as the online_dqn (it isn't needed for testing).\n",
    " #   pass\n",
    "\n",
    "#agent = AtariAgent(buffer=buffer, eps=eps, eps_decay=eps_decay, min_eps=min_eps, gamma=gamma, batch_size=batch_size,\n",
    " #                  online_dqn=online_dqn, target_dqn=target_dqn, run_as_ddqn=run_as_ddqn,\n",
    "  #                 optimizer=optimizer, criterion=criterion, device=device, \n",
    "   #                max_train_frames=max_train_frames, burn_in_phase=burn_in_phase, sync_target=sync_target)\n",
    "\n",
    "#if testing_mode:\n",
    " #   test_metrics = dict(reward=[], loss=[])\n",
    "  #  for it in range(max_test_episodes):\n",
    "   #     episode_metrics = agent.run_episode(is_training=False)\n",
    "    #    update_metrics(test_metrics, episode_metrics)\n",
    "     #   print_metrics(it + 1, test_metrics, is_training=False)\n",
    "#else:\n",
    " #   train_metrics = dict(reward=[], loss=[])\n",
    "  #  for it in range(max_train_episodes):\n",
    "   #     episode_metrics = agent.run_episode(is_training=True)\n",
    "    #    update_metrics(train_metrics, episode_metrics)\n",
    "     #   if it % 50 == 0:\n",
    "      #      print_metrics(it, train_metrics, is_training=True)\n",
    "       #     agent.save_checkpoint(train_metrics, save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Preprocessing Variations (EXTRA)\n",
    "# -------------------------------\n",
    "preprocessing_variations = [\n",
    "    {\"num_skip\": 4, \"resize_shape\": 84, \"grayscale\": True},   # Default setup\n",
    "    {\"num_skip\": 4, \"resize_shape\": 96, \"grayscale\": True},   # Larger frame size\n",
    "    {\"num_skip\": 2, \"resize_shape\": 84, \"grayscale\": False},  # RGB input with fewer skips\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameters and Configuration\n",
    "# -------------------------------\n",
    "\n",
    "# Base hyperparameters\n",
    "base_hyperparams = {\n",
    "    \"batch_size\": 32,\n",
    "    \"alpha\": 0.00025,\n",
    "    \"gamma\": 0.95,\n",
    "    \"eps\": 1.0,\n",
    "    \"eps_decay\": 0.999,\n",
    "    \"min_eps\": 0.05,\n",
    "    \"buffer_size\": 20_000,\n",
    "    \"burn_in_phase\": 20_000,\n",
    "    \"sync_target\": 30_000,\n",
    "    \"max_train_frames\": 10_000,\n",
    "    \"max_train_episodes\": 100_000,\n",
    "    \"max_test_episodes\": 1,\n",
    "    \"run_as_ddqn\": False,  # Toggle for Double DQN\n",
    "    \"save_filename\": \"./saved_model.pt\",\n",
    "}\n",
    "\n",
    "# Refined Hyperparameter Variations (EXTRA)\n",
    "hyperparameter_variations = {\n",
    "    \"batch_size\": [32, 64],           # Small and large batches for gradient updates\n",
    "    \"alpha\": [0.00025, 0.0005],       # Adjusted to focus on faster learning rates\n",
    "    \"eps_decay\": [0.995, 0.999],      # Slightly different exploration strategies\n",
    "    \"burn_in_phase\": [10_000, 20_000],# Small and large replay buffer warm-up sizes\n",
    "    \"sync_target\": [10_000, 30_000],  # Frequent and infrequent target updates\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Create Replay Buffer, Optimizer, and Loss\n",
    "# -------------------------------\n",
    "buffer = ExperienceReplayMemory(base_hyperparams[\"buffer_size\"])\n",
    "optimizer = torch.optim.Adam(online_dqn.parameters(), lr=base_hyperparams[\"alpha\"])  # DONE\n",
    "criterion = torch.nn.MSELoss()  # DONE\n",
    "\n",
    "# -------------------------------\n",
    "# Training Mode with Preprocessing and Hyperparameter Search\n",
    "# -------------------------------\n",
    "if not testing_mode:\n",
    "    results = []  # Store results for multiple configurations\n",
    "    \n",
    "    # Loop through preprocessing variations\n",
    "    for preprocess_config in preprocessing_variations:  # EXTRA\n",
    "        print(f\"Using preprocessing config: {preprocess_config}\")  # EXTRA: Log preprocessing config\n",
    "        \n",
    "        # Create and preprocess environment\n",
    "        env = gym.make(\"ALE/Breakout-v5\", full_action_space=False)\n",
    "        env = SkipFrame(env, num_skip=preprocess_config[\"num_skip\"])\n",
    "        if preprocess_config[\"grayscale\"]:\n",
    "            env = GrayScaleObservation(env)\n",
    "        env = ResizeObservation(env, shape=preprocess_config[\"resize_shape\"])\n",
    "        env = FrameStackObservation(env, stack_size=4)  # Fixed stack size\n",
    "\n",
    "        # Extract environment dimensions\n",
    "        image_stack, h, w = env.observation_space.shape\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Loop through hyperparameter configurations\n",
    "        for batch_size in hyperparameter_variations[\"batch_size\"]:\n",
    "            for alpha in hyperparameter_variations[\"alpha\"]:\n",
    "                for eps_decay in hyperparameter_variations[\"eps_decay\"]:\n",
    "                    for burn_in_phase in hyperparameter_variations[\"burn_in_phase\"]:\n",
    "                        for sync_target in hyperparameter_variations[\"sync_target\"]:\n",
    "                            print(f\"Training with batch_size={batch_size}, alpha={alpha}, eps_decay={eps_decay}, \"\n",
    "                                  f\"burn_in_phase={burn_in_phase}, sync_target={sync_target}\")\n",
    "\n",
    "                            # Update agent hyperparameters dynamically\n",
    "                            optimizer = torch.optim.Adam(online_dqn.parameters(), lr=alpha)  # EXTRA\n",
    "                            agent = AtariAgent(\n",
    "                                buffer=buffer,\n",
    "                                eps=base_hyperparams[\"eps\"],\n",
    "                                eps_decay=eps_decay,\n",
    "                                min_eps=base_hyperparams[\"min_eps\"],\n",
    "                                gamma=base_hyperparams[\"gamma\"],\n",
    "                                batch_size=batch_size,\n",
    "                                online_dqn=online_dqn,\n",
    "                                target_dqn=target_dqn,\n",
    "                                run_as_ddqn=base_hyperparams[\"run_as_ddqn\"],\n",
    "                                optimizer=optimizer,\n",
    "                                criterion=criterion,\n",
    "                                device=device,\n",
    "                                max_train_frames=base_hyperparams[\"max_train_frames\"],\n",
    "                                burn_in_phase=burn_in_phase,\n",
    "                                sync_target=sync_target,\n",
    "                            )\n",
    "\n",
    "                            # Training loop\n",
    "                            train_metrics = {\"reward\": [], \"loss\": []}\n",
    "                            for it in range(base_hyperparams[\"max_train_episodes\"]):\n",
    "                                episode_metrics = agent.run_episode(is_training=True)\n",
    "                                update_metrics(train_metrics, episode_metrics)\n",
    "                                if it % 50 == 0:\n",
    "                                    print_metrics(it, train_metrics, is_training=True)\n",
    "                                    agent.save_checkpoint(train_metrics, base_hyperparams[\"save_filename\"])\n",
    "\n",
    "                            # Store results\n",
    "                            results.append({\n",
    "                                \"preprocess_config\": preprocess_config,  # EXTRA\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"alpha\": alpha,\n",
    "                                \"eps_decay\": eps_decay,\n",
    "                                \"burn_in_phase\": burn_in_phase,\n",
    "                                \"sync_target\": sync_target,\n",
    "                                \"metrics\": train_metrics,\n",
    "                            })\n",
    "\n",
    "# -------------------------------\n",
    "# Testing Results (if applicable)\n",
    "# -------------------------------\n",
    "if testing_mode:\n",
    "    test_metrics = {\"reward\": [], \"loss\": []}\n",
    "    for it in range(base_hyperparams[\"max_test_episodes\"]):\n",
    "        episode_metrics = agent.run_episode(is_training=False)\n",
    "        update_metrics(test_metrics, episode_metrics)\n",
    "        print_metrics(it + 1, test_metrics, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy7C4qfWkzXb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot metrics\n",
    "def plot_metrics(metrics, title=\"Metrics Over Episodes\"):\n",
    "    \"\"\"\n",
    "    Plots reward and loss metrics over episodes.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): A dictionary containing \"reward\" and \"loss\" lists.\n",
    "        title (str): The title for the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.plot(metrics[\"reward\"], label=\"Reward\", color=\"blue\")\n",
    "    \n",
    "    # Plot losses\n",
    "    if \"loss\" in metrics and len(metrics[\"loss\"]) > 0:\n",
    "        plt.plot(metrics[\"loss\"], label=\"Loss\", color=\"red\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Plot train metrics\n",
    "plot_metrics(train_metrics, title=\"Training Metrics\")\n",
    "\n",
    "# Plot test metrics if available\n",
    "if testing_mode and \"test_metrics\" in locals():\n",
    "    plot_metrics(test_metrics, title=\"Testing Metrics\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
