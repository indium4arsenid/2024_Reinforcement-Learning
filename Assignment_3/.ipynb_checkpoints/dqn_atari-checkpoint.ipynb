{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rr1d-kHohxGL"},"outputs":[],"source":["%pip install \"gymnasium[atari,accept-rom-license]==1.0.0\" \"ale-py==0.9.1\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import sys, os\n","import gymnasium as gym\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import numpy as np\n","import random\n","from gymnasium.spaces import Box\n","from collections import deque\n","import copy\n","from gymnasium.wrappers import FrameStackObservation\n","import ale_py\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IGCa_JQeiy1F"},"outputs":[],"source":["class SkipFrame(gym.Wrapper):\n","    def __init__(self, env, num_skip):\n","        super().__init__(env)\n","        self.num_skip = num_skip\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        for _ in range(self.num_skip):\n","            obs, reward, terminated, truncated, info = self.env.step(action)\n","            total_reward += reward\n","            if terminated or truncated:\n","                break\n","\n","        return obs, total_reward, terminated, truncated, info\n","\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.float32)\n","\n","    def observation(self, observation):\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","        transform = torchvision.transforms.Grayscale()\n","        observation = transform(observation)\n","        return observation\n","\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.float32)\n","\n","    def observation(self, observation):\n","        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n","                                                     torchvision.transforms.Normalize(0, 255)])\n","        return transforms(observation).squeeze(0)\n","\n","\n","class ExperienceReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","    def store(self, state, next_state, action, reward, terminated, truncated):\n","        state = state.__array__()\n","        next_state = next_state.__array__()\n","        self.memory.append((state, next_state, action, reward, terminated, truncated))\n","\n","    def sample(self, batch_size):\n","        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, terminated, truncated\n","        # ...\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOCiUgfBjJYc"},"outputs":[],"source":["seed = 957\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.backends.cudnn.enabled:\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","gym.register_envs(ale_py)\n","\n","env_rendering = False    # Set to False while training your model on Colab\n","\n","# Create and preprocess the Atari Breakout environment\n","if env_rendering:\n","    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False, render_mode=\"human\")\n","else:\n","    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False)\n","\n","env = SkipFrame(env, num_skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)\n","env = FrameStackObservation(env, stack_size=4)\n","\n","image_stack, h, w = env.observation_space.shape\n","num_actions = env.action_space.n\n","print(f'Number of stacked frames: {image_stack}')\n","print(f'Resized observation space dimensionality: {h}, {w}')\n","print(f'Number of available actions by the agent: {num_actions}')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNoUgNjujqRX"},"outputs":[],"source":["class DeepQNet(torch.nn.Module):\n","    def __init__(self, h, w, image_stack, num_actions):\n","        super(DeepQNet, self).__init__()\n","        # TODO: create a convolutional neural network\n","        pass\n","\n","    def forward(self, x):\n","        # TODO: forward pass from the neural network\n","        pass\n","\n","\n","# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and \n","#       set requires_grad to False for the parameters of the target DQN)\n","online_dqn = ...\n","target_dqn = ...\n","online_dqn.to(device)\n","target_dqn.to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RO21LQJ6j0WC"},"outputs":[],"source":["def convert(x):\n","    return torch.tensor(x.__array__()).float()\n","\n","\n","class AtariAgent:\n","    def __init__(self, buffer, eps, eps_decay, min_eps, gamma, batch_size,\n","                 online_dqn, target_dqn, run_as_ddqn, \n","                 optimizer, criterion, device,\n","                 max_train_frames, burn_in_phase, sync_target):\n","\n","        self.buffer = buffer\n","        self.eps = eps\n","        self.eps_decay = eps_decay\n","        self.min_eps = min_eps\n","        self.gamma = gamma\n","        self.batch_size = batch_size\n","\n","        self.online_dqn = online_dqn\n","        self.target_dqn = target_dqn\n","        self.run_as_ddqn = run_as_ddqn\n","        self.optimizer = optimizer\n","        self.criterion = criterion\n","        self.device = device\n","        self.max_train_frames = max_train_frames\n","        self.burn_in_phase = burn_in_phase\n","        self.sync_target = sync_target\n","\n","        self.current_step = 0\n","\n","\n","    def policy(self, state, is_training):\n","        state = convert(state).unsqueeze(0).to(self.device)\n","\n","        # TODO: Implement an epsilon-greedy policy\n","        pass\n","\n","\n","    def compute_loss(self, state, action, reward, next_state, truncated, terminated):\n","        state, action, reward, next_state, truncated, terminated = [x.to(self.device) for x in \n","                                (state, action, reward, next_state, truncated, terminated)]\n","\n","        # TODO: Compute the DQN (or DDQN) loss based on self.criterion\n","        pass\n","\n","\n","    def run_episode(self, is_training):\n","        episode_reward, episode_loss = 0, 0.\n","        state, _ = env.reset(seed=seed)\n","\n","        for t in range(self.max_train_frames):\n","            action = self.policy(state, is_training)\n","            self.current_step += 1\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","\n","            episode_reward += reward\n","\n","            if is_training:\n","                self.buffer.store(state, next_state, action, reward, terminated, truncated)\n","\n","                if self.current_step > self.burn_in_phase:\n","                    state_batch, next_state_batch, action_batch, \\\n","                        reward_batch, terminated_batch, truncated_batch = self.buffer.sample(self.batch_size)\n","\n","                    if self.current_step % self.sync_target == 0:\n","                        # TODO: Periodically update your target_dqn at each sync_target frames\n","                        pass\n","\n","                    loss = self.compute_loss(state_batch, action_batch, reward_batch, \n","                                             next_state_batch, terminated_batch, truncated_batch)\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    episode_loss += loss.detach().item()\n","            else:\n","                with torch.no_grad():\n","                    st = convert(state).to(self.device).unsqueeze(0)\n","                    next_st = convert(next_state).to(self.device).unsqueeze(0)\n","                    act = action.to(self.device)\n","                    rew = torch.tensor(reward).to(self.device)\n","                    trunc = torch.tensor(truncated).to(self.device)\n","                    term = torch.tensor(terminated).to(self.device)\n","\n","                    episode_loss += self.compute_loss(st, act, rew, next_st, term, trunc).item()\n","\n","            state = next_state\n","\n","            if self.current_step > self.burn_in_phase and self.eps > self.min_eps:\n","                self.eps *= self.eps_decay\n","\n","            if terminated or truncated:\n","                break\n","\n","        return dict(reward=episode_reward, loss=episode_loss / t)\n","\n","\n","    def save_checkpoint(self, train_metrics, save_filename):\n","        save_dict = {'curr_step': self.current_step,\n","                    'train_metrics': train_metrics,\n","                    'eps': self.eps,\n","                    'online_dqn': self.online_dqn.state_dict(),\n","                    'target_dqn': self.target_dqn.state_dict()}\n","\n","        torch.save(save_dict, save_filename)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"PEGSGYsQj8Wh"},"outputs":[],"source":["def update_metrics(metrics, episode):\n","    for k, v in episode.items():\n","        metrics[k].append(v)\n","\n","\n","def print_metrics(it, metrics, is_training, window=100):\n","    reward_mean = np.mean(metrics['reward'][-window:])\n","    loss_mean = np.mean(metrics['loss'][-window:])\n","    mode = \"train\" if is_training else \"test\"\n","    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdhWcoW-kyuF"},"outputs":[],"source":["# Hyperparameters (TODO: modify as needed)\n","batch_size = 32\n","alpha = 0.00025\n","gamma = 0.95\n","eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n","buffer = ExperienceReplayMemory(20_000)\n","burn_in_phase = 20_000\n","sync_target = 30_000\n","max_train_frames = 10_000\n","max_train_episodes = 100_000\n","max_test_episodes = 1\n","run_as_ddqn = False # Set the run_as_ddqn flag to True if you want to run the DDQN algorithm\n","save_filename = './saved_model.pt'\n","\n","# TODO: create the appropriate MSE criterion and Adam optimizer\n","optimizer = ...\n","criterion = ...\n","\n","testing_mode = False # Change to True if you want to load a saved model\n","\n","if testing_mode:\n","    # TODO: Load your saved online_dqn model for testing. \n","    #       The target_dqn should be the same as the online_dqn (it isn't needed for testing).\n","    pass\n","\n","agent = AtariAgent(buffer=buffer, eps=eps, eps_decay=eps_decay, min_eps=min_eps, gamma=gamma, batch_size=batch_size,\n","                   online_dqn=online_dqn, target_dqn=target_dqn, run_as_ddqn=run_as_ddqn,\n","                   optimizer=optimizer, criterion=criterion, device=device, \n","                   max_train_frames=max_train_frames, burn_in_phase=burn_in_phase, sync_target=sync_target)\n","\n","if testing_mode:\n","    test_metrics = dict(reward=[], loss=[])\n","    for it in range(max_test_episodes):\n","        episode_metrics = agent.run_episode(is_training=False)\n","        update_metrics(test_metrics, episode_metrics)\n","        print_metrics(it + 1, test_metrics, is_training=False)\n","else:\n","    train_metrics = dict(reward=[], loss=[])\n","    for it in range(max_train_episodes):\n","        episode_metrics = agent.run_episode(is_training=True)\n","        update_metrics(train_metrics, episode_metrics)\n","        if it % 50 == 0:\n","            print_metrics(it, train_metrics, is_training=True)\n","            agent.save_checkpoint(train_metrics, save_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uy7C4qfWkzXb"},"outputs":[],"source":["# TODO: Plot your train_metrics and test_metrics"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
